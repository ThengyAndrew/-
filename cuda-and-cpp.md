# CUDA & cpp

### å‚è€ƒèµ„æ–™

* [https://zhuanlan.zhihu.com/p/462822421](https://zhuanlan.zhihu.com/p/462822421)



#### é¦–ä¸ªä»£ç 

{% code lineNumbers="true" %}
```cpp
// freshman.hå¤´æ–‡ä»¶
#ifndef FRESHMAN_H
#define FRESHMAN_H
// æ£€æŸ¥è¿”å›ç»“æœcudaError_tæ˜¯å¦æˆåŠŸ
#define CHECK(call)\   
{\
  const cudaError_t error=call;\
  if(error!=cudaSuccess)\
  {\
      printf("ERROR: %s:%d,",__FILE__,__LINE__);\
      printf("code:%d,reason:%s\n",error,cudaGetErrorString(error));\
      exit(1);\
  }\
}
#endif

// SumArrays.cuæºä»£ç æ–‡ä»¶
#include <cuda_runtime.h> // cudaç›¸å…³å¤´æ–‡ä»¶
#include <stdio.h>
#include "freshman.h"

void initialData(float* ip,int size)
{
  time_t t;
  srand((unsigned )time(&t));
  for(int i=0;i<size;i++)
  {
    ip[i]=(float)(rand()&0xffff)tf("\n");
  }
}

__global__ void sumArraysCuda(float*a,float*b,float*res)
{
  int i=threadIdx.x;
  res[i]=a[i]+b[i];
}
int main(int argc,char **argv)
{
  int dev = 0;
  cudaSetDevice(dev); // å½“æœ‰å¤šä¸ªGPUæ—¶ï¼Œé€‰å®šcudaè®¾å¤‡ï¼Œé»˜è®¤æ˜¯0å³ç¬¬ä¸€ä¸ªä¸»GPUï¼Œå¤šGPUæ—¶0,1,2ä»¥æ­¤ç±»æ¨

  int nElem=32;
  printf("Vector size:%d\n",nElem);
  int nByte=sizeof(float)*nElem;
  float *a_h=(float*)malloc(nByte);
  float *b_h=(float*)malloc(nByte);
  float *res_h=(float*)malloc(nByte);
  memset(res_h,0,nByte);


// åœ¨ GPU ä¸­ç»™ä¸‰ä¸ªæ•°ç»„ç”³è¯·å†…å­˜ç©ºé—´
  float *a_d,*b_d,*res_d;
  CHECK(cudaMalloc((float**)&a_d,nByte));
  CHECK(cudaMalloc((float**)&b_d,nByte));
  CHECK(cudaMalloc((float**)&res_d,nByte));

  initialData(a_h,nElem); 
  initialData(b_h,nElem);

  CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice)); // ä»ä¸»æœºå†…å­˜æ‹·è´åˆ°è®¾å¤‡å†…å­˜
  CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice)); 

// æ ¸å¿ƒ
  dim3 block(nElem); // 1 ä¸ª block 32 ä¸ª Thread
  dim3 grid(nElem/block.x); // å…ƒç´ æ€»æ•°/å•ä¸ª bl çš„ Th æ•° = ä¸€ä¸ª grid çš„ block æ•°
  sumArraysCuda<<<grid,block>>>(a_d,b_d,res_d);
  printf("Execution configuration<<<%d,%d>>>\n",block.x,grid.x);

  CHECK(cudaMemcpy(res_h,res_d,nByte,cudaMemcpyDeviceToHost)); // å°†ç»“æœä»è®¾å¤‡å†…å­˜æ‹·è´åˆ°ä¸»æœºå†…å­˜ï¼
  cudaFree(a_d);
  cudaFree(b_d);
  cudaFree(res_d);

  free(a_h);
  free(b_h);
  free(res_h);

  return 0;
}\
```
{% endcode %}

{% hint style="info" %}
è¿™é‡Œä¸ºä»€ä¹ˆè‡ªåŠ¨è¿›è¡Œäº† 32 ä¸ªå…ƒç´ çš„è¿ç®—?

1 block starts 32 Threads, and 1 id for 1 Thread.
{% endhint %}

#### grid-block-thread çš„åˆ†åˆ’å¾ˆåƒå­˜å‚¨ç³»ç»Ÿ

| BlockIdx.x | ThreadIdx.x | Global idx |
| ---------- | ----------- | ---------- |
| 0          | 0 \~ 31     | 0 \~ 31    |
| 1          | 0 \~ 31     | 32 \~ 63   |
| 2          | 0 \~ 31     | 64 \~ 95   |
| 3          | 0 \~ 31     | 96 \~ 127  |



### GPU çš„ç¡¬ä»¶ï¼ˆåŸºäº Fermi æ¶æ„ï¼‰

```
+-------------------------------------------------------------+
|                        GPU Device                           |
|                                                             |
|  +--------+   +--------+   ...   +--------+                 |
|  |   SM0  |   |   SM1  |         |   SM15 |  â† å¤šä¸ª SMï¼ˆæµå¼å¤šå¤„ç†å™¨ï¼‰|
|  +--------+   +--------+         +--------+                 |
|     â†‘            â†‘                   â†‘                      |
|   æ¯ä¸ª SM æœ‰å¤šä¸ª CUDA Coreã€å¯„å­˜å™¨ã€å…±äº«å†…å­˜                 |
|                                                             |
|                      GigaThread Engine                      |
|                ï¼ˆè°ƒåº¦çº¿ç¨‹å—ç»™ SM çš„å¤§ç®¡å®¶ï¼‰                 |
|                                                             |
|                  å…¨å±€å†…å­˜ï¼ˆglobal DRAMï¼‰                    |
+-------------------------------------------------------------+
```

#### <mark style="background-color:purple;">SMï¼ˆStreaming Multiprocessorï¼‰æµå¼å¤šå¤„ç†å™¨</mark>

æ¯ä¸ª SM ç±»ä¼¼ä¸€ä¸ªâ€œå¾®å‹å¹¶è¡Œå¤„ç†å™¨é˜µåˆ—â€ï¼Œæ‹¥æœ‰ï¼š

* å¤šä¸ª <mark style="background-color:purple;">**CUDA Coreï¼ˆæ•´æ•°/æµ®ç‚¹å¤„ç†å•å…ƒï¼‰**</mark>
  * æ¯ä¸ª SM å†…éƒ¨æœ‰å¤šä¸ª CUDA Coreï¼ˆä¾‹å¦‚ 128 ä¸ªï¼‰
  * ä¸€ä¸ª CUDA Core ç›¸å½“äºå¤„ç†ä¸€ä¸ª thread çš„æœ€å°å•ä½
  * ä½†æ˜¯ CUDA æ˜¯ä»¥ 32 ä¸ªçº¿ç¨‹ä¸ºå•ä½è°ƒåº¦ï¼ˆwarpï¼‰
* å…±äº«å†…å­˜ï¼ˆblock å†…çº¿ç¨‹å…±äº«ï¼‰
  * æ¯ä¸ª SM è‡ªå¸¦çš„å°å‹é«˜é€Ÿç¼“å­˜ï¼ˆå‡  KB åˆ° 100 KBï¼‰
  * block å†…çš„çº¿ç¨‹å¯ä»¥å…±äº«è®¿é—®
  * è®¿é—®é€Ÿåº¦æ¯”å…¨å±€å†…å­˜å¿«å¾—å¤š
  * é€‚åˆçº¿ç¨‹é—´é€šä¿¡æˆ–ç¼“å­˜ä¸­é—´ç»“æœ
* å¯„å­˜å™¨ç»„ï¼ˆçº¿ç¨‹ç§æœ‰å˜é‡ï¼‰
* warp è°ƒåº¦å™¨ï¼ˆè°ƒåº¦ 32 ä¸ªçº¿ç¨‹ä¸€èµ·æ‰§è¡Œï¼‰
* ä¸€ä¸ªçº¿ç¨‹å—ï¼ˆblockï¼‰è¢«åˆ†é…åˆ°ä¸€ä¸ª SM ä¸Šåï¼ŒSM å†…çš„ç¡¬ä»¶ä¼šç®¡ç†å…¶ä¸­æ‰€æœ‰çº¿ç¨‹çš„æ‰§è¡Œã€‚

ä¸€ä¸ª GPU é€šå¸¸æœ‰ 16ã€48ã€80 ç”šè‡³ä¸Šç™¾ä¸ª SMï¼ˆå¦‚ A100 æœ‰ 108 ä¸ª SMï¼‰



#### GigaThread Engineï¼ˆè¶…çº¿ç¨‹å¼•æ“ï¼‰

è¿™æ˜¯ä¸€ä¸ª **å…¨å±€è°ƒåº¦å™¨**ï¼Œåœ¨ Fermi æ¶æ„ä¸­é¦–æ¬¡å‡ºç°ã€‚

å®ƒçš„åŠŸèƒ½ï¼š

* åœ¨ kernel å¯åŠ¨æ—¶ï¼Œå°†çº¿ç¨‹å—åˆ†é…åˆ°åˆé€‚çš„ SM ä¸Š
* å½“æŸä¸ª SM ç©ºé—²æ—¶ï¼Œç»§ç»­ä»å¾…åˆ†é…é˜Ÿåˆ—ä¸­æ‹‰å–ä¸‹ä¸€ä¸ª block
* ä¸ç®¡ä½ æœ‰ 1000 ä¸ª blockï¼Œå“ªå‡ ä¸ªèƒ½åŒæ—¶æ‰§è¡Œï¼Œæ˜¯ç”± GigaThread å†³å®šçš„

ğŸ‘‰ **GigaThread ä¸è°ƒåº¦å•ä¸ªçº¿ç¨‹ï¼Œåªè°ƒåº¦çº¿ç¨‹å—ï¼ˆblockï¼‰ï¼**



#### DRAMï¼ˆGlobal Memoryï¼Œå…¨å±€å†…å­˜ï¼‰

* æ˜¯ GPU çš„æ˜¾å­˜ï¼ˆå¦‚ RTX 3060 æœ‰ 12GBï¼‰
* æ‰€æœ‰çº¿ç¨‹éƒ½å¯ä»¥è®¿é—®ï¼Œä½†**è®¿é—®æ…¢**
* é€‚åˆå­˜æ”¾å¤§æ•°æ®ï¼Œæ¯”å¦‚è¾“å…¥çŸ©é˜µã€è¾“å‡ºç»“æœ

### ç‰¹æ€§ä¸æŠ€å·§

#### è®¿å­˜åˆå¹¶

pytorch çš„æ¨¡å¼:  <mark style="background-color:blue;">**row-major**</mark>

#### å‘é‡åŒ–è®¿å­˜ vectorized

| ç±»å‹           | å¯¹åº”ä½å®½    |
| ------------ | ------- |
| `float2`     | 64-bit  |
| `float4`     | 128-bit |
| `int2`       | 64-bit  |
| `int4`       | 128-bit |
| `half2`      | 32-bit  |
| `__half2`    | 32-bit  |
| `bfloat162`  | 32-bit  |
| `bfloat168*` | 128-bit |
| `char4`      | 32-bit  |
| `uchar4`     | 32-bit  |
| `ushort2`    | 32-bit  |
| è‡ªå®šä¹‰ç»“æ„ä½“       | ä»»æ„      |

ç¤ºä¾‹ï¼š
{% code lineNumbers="true" %}
```cpp
__global__ void silu_coalesced_float4_kernel(const float* input, float* output, int rows, int cols) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col4 = blockIdx.x * blockDim.x + threadIdx.x; // col4 æ˜¯é€»è¾‘ä¸Šçš„åˆ—ç¼–å·ï¼Œæ¯ä¸ªå•ä½å¯¹åº”ä¸€ä¸ª float4ï¼ˆ4 ä¸ªå…ƒç´ ï¼‰
    int col = col4 << 2;

    if (row < rows && col + 3 < cols) {
        const float4* input_v4 = reinterpret_cast<const float4*>(input); // é€šè¿‡ reinterpret_cast æŠŠ float* æŒ‡é’ˆå¼ºè½¬ä¸º float4* æŒ‡é’ˆï¼ˆæ¯æ¬¡è®¿é—® 16 å­—èŠ‚ï¼‰
        float4 val = input_v4[row * (cols >> 2) + col4]; // ç¬¬ row è¡Œã€ç¬¬ col4 ä¸ª float4 çš„åç§»é‡æ˜¯ row * (cols >> 2) + col4

        val.x = silu(val.x);
        val.y = silu(val.y);
        val.z = silu(val.z);
        val.w = silu(val.w);

        float4* output_v4 = reinterpret_cast<float4*>(output);
        output_v4[row * (cols >> 2) + col4] = val;
    }
}
```
{% endcode %}

#### SFU å•å…ƒåŠ é€Ÿ

| å‡½æ•°                 | æè¿°             |
| ------------------ | -------------- |
| `__sinf(x)`        | å¿«é€Ÿè®¡ç®— `sin(x)`  |
| `__cosf(x)`        | å¿«é€Ÿè®¡ç®— `cos(x)`  |
| `__expf(x)`        | å¿«é€Ÿè®¡ç®— `exp(x)`  |
| `__logf(x)`        | å¿«é€Ÿè®¡ç®— `log(x)`  |
| `__powf(x, y)`     | å¿«é€Ÿè®¡ç®— `x^y`     |
| `__sqrtf(x)`       | å¿«é€Ÿè®¡ç®— `sqrt(x)` |
| `__fdividef(a, b)` | å¿«é€Ÿé™¤æ³•           |

